#!/usr/bin/env python

'''
    Usage:
           $ python run_admd.py [name] [additional options]

'''

import os
_cwd_ = os.getcwd()

import sys
import time
import shutil
import glob
import datetime
import traceback
from pprint import pformat

from adaptivemd import PythonTask, Task
from adaptivemd.file import URLGenerator
from adaptivemd.runtime import get_argparser, initialize_project, workflow_generator_simple, create_workload_launcher
from adaptivemd.util import get_logger
logger = get_logger(logname=__name__)


# Exit codes from this script are NOT conventional!
#
# negative: an error
#
# zero:     no error
#
# positive: the integer number of tasks that were found to
#           have (erroneously) not completed

# FIXME these should be organized within the AdaptiveMD Task object
final_states    = Task.FINAL_STATES + Task.RESTARTABLE_STATES
created_state   = 'created'
created_update  = {"state":created_state, "__dict__.state":created_state}
fix_states      = {'queued','running','fail','failed','cancelled','pending'}
runnable_states = set(fix_states)
runnable_states.add(created_state)

task_done = lambda ta: ta.state in final_states
is_incomplete = lambda ta: ta.state in runnable_states

session = None

if __name__ == '__main__':

    project = None
    sleeptime = 1

    try:

        _exitval_ = -1

        parser   = get_argparser()
        args     = parser.parse_args()

        submit_only = args.submit_only

        logger.info("Initializing Project named: " + args.project_name)
        logger.info("Recieved Arguments:\n{}".format(args))
        logger.info("Project opening")
        logger.debug("with configs from file    : %s"%args.config)
        logger.debug("and selecting config named: %s"%args.config_name)

        project = initialize_project(
            args.project_name,
            sys_name = args.system_name,
            m_freq   = args.all,
            p_freq   = args.prot,
            platform = args.platform,
            features = args.features_cfg,
            config_file = args.config,
            config_name = args.config_name,
        ) 

        logger.debug("Project opened")
        logger.info(
            "AdaptiveMD dburl: {}".format(project.storage._db_url))

        logger.info(
            "number of project.trajectories: {}".format(
                len(project.trajectories)))

        logger.info(
            "number of project.models: {}".format(
                len(project.models)))

        if not args.rescue_only and not args.init_only:
            sessions = URLGenerator("sessions/{count:06}")
            # FIXME this isn't working to get next one
            if os.path.exists("sessions"):
                sessions.initialize_from_files([
                    os.path.join("sessions", d) for d in os.listdir("sessions")])

            else:
                os.makedirs("sessions")

            for d in os.listdir("sessions"):
                next(sessions)

            session = next(sessions)
            os.makedirs(session)

        n_incomplete_tasks = 0
        if args.rescue_tasks or args.rescue_only:

            logger.info("Checking for pre-existing, runnable tasks")
            rescue_tasks = list(filter(is_incomplete, project.tasks))
            rescue_uuids = list(map(lambda ta: ta.__uuid__, rescue_tasks))
            n_incomplete_tasks = len(rescue_tasks)
            logger.info("Found %d incomplete tasks to run" % n_incomplete_tasks)

            if n_incomplete_tasks and not args.rescue_only:

                # Rescue tasks without rescue only uses
                # multijob control flow
                _exitval_ = n_incomplete_tasks
                logger.info(
                  "Exiting to launch job for rescueing {} incomplete tasks".format(_exitval_)
                )

            elif args.rescue_only:

                # Rescue tasks with rescue only means this
                # job cycle does execution (ie no exit)
                _exitval_ = n_incomplete_tasks
                logger.info("Continuing to execute runnable incomplete tasks")

            else:
                logger.info(
                    "Proceeding after rescue check, found {} incomplete tasks".format(
                    n_incomplete_tasks)
                )

        else:
            logger.info(
                "No check performed to see if there are incomplete tasks"
            )

        if args.init_only:
            logger.info("Leaving project '{}' initialized without tasks".format(
                project.name)
            )

            _exitval_ = 0

        else:
            logger.info("Configuring workload")

            n_traj    = 0
            modeller  = None
            n_rounds  = 1
            new_tasks = list()

            if not args.rescue_only and not n_incomplete_tasks:
                logger.info("Setting up for newly issued tasks")

                # TODO we currently only support a single simulation configuration
                #      with the runtime system.
                engine   = project.generators["openmm"]
                n_traj   = args.n_traj
                n_rounds = args.n_rounds # n_rounds in single runtime, almost always 1
                round_n  = args.round_n  # round number for overall workflow
                length   = args.length

                # We support multiple modellers so that analysis using
                # different featurization can be evaluated easily
                if args.modeller:
                    nm_modeller = args.modeller
                    modeller = project.generators[nm_modeller]
                    analysis_cfg = args.analysis_cfg

            # TODO FIXME this else catches new trajs + incomplete trajs case and
            #            just ignores the new trajs for configuring workload
            elif n_incomplete_tasks:
                logger.info(
                    "Looking to clean up unfinished tasks: task states: {}".format(
                    project.task_states))

                # TODO FIXME this approach for resetting task states not acceptable for general use...
                #            in the future a single adaptivemd project instance should be able to
                #            coordinate multiple workloads simultaneously, and this introduces race
                #            conditions since we are in no way trying to target the reset action to
                #            specific groups of tasks, i.e. by their resource, round number (isn't
                #            stored yet anyways), etc
                for fix_state in fix_states:
                    project.tasks._set._document.update_many(
                        {"state": fix_state},
                        {"$set" : created_update}
                    )

                logger.info(
                    "After fixes: task states: {}".format(
                    project.task_states))

                #project.tasks._set.clear_cache()
                #project.tasks._set.load_indices()
                #logger.info("After reload: observed task states: {}".format(project.task_states))

                new_tasks = project.tasks.v(
                    lambda ta: ta.__uuid__ in rescue_uuids)

                model_tasks = list(new_tasks.c(PythonTask))

                # FIXME list elements getting duplicated somehow
                # TODO  see if this still happens ^^^
                new_tasks = list(set(new_tasks))
                model_tasks = list(set(model_tasks))
                logger.info(
                    "Found {} tasks to execute in cleanup".format(
                    len(new_tasks))
                )

                if not new_tasks:

                    logger.info((
                        "Checked for failed tasks to rescue but "
                        "found none, exiting without error"
                    ))

                    _exitval_ = 0

                #for ta in new_tasks:
                #    logger.info("{0}  {1}".format(ta.state, getattr(ta, 'trajectory', None)))

                # FIXME get the number of MD steps to be run in the task, not total MD length
                #       and use this for the length argument to generator function
                #if any([hasattr(ta, 'trajectory') for ta in new_tasks]):
                #    # doesn't handle TrajectoryExtensionTask correctly
                #    length    = max(map(lambda ta: ta.trajectory.length, filter(lambda ta: hasattr(ta,'trajectory'), new_tasks)))

            logger.info("n_rounds: {}".format(n_rounds))

            if n_traj or modeller:

                # We're going to track what happens with new tasks that
                # get generated next, need to capture current inventory
                existing_tasks = [ta.__uuid__ for ta in project.tasks]
                logger.info(
                    "Project adding event from {}".format(
                    workflow_generator_simple))

                project.add_event(workflow_generator_simple(
                    project, engine, n_traj, length, round_n,
                    longest = args.all,
                    n_rounds = n_rounds,
                    modeller = modeller,
                    minlength = args.minlength,
                    batchsize = args.batchsize,
                    batchwait = args.batchwait,
                    batchsleep = args.batchsleep,
                    progression = args.progression,
                    cpu_threads = args.threads,
                    fixedlength = True,#args.fixedlength,
                    startontraj = args.after_n_trajs,
                    admd_profile = None,#args.config,
                    analysis_cfg = args.analysis_cfg,
                    min_model_trajlength = args.min_model_trajlength,
                    sampling_function_name = args.sampling_method,
                ))

                new_tasks = list(filter(
                    lambda ta: ta.__uuid__ not in existing_tasks, project.tasks))

            elif new_tasks:

                logger.info(
                    "Project event adding from incomplete tasks\n{}".format(
                    new_tasks)
                )

                sleeptime = 20
                logger.debug("These are the tasks being cleaned up:")
                logger.debug(pformat(new_tasks))

                def event_new_tasks():
                    yield lambda: all([ta.is_done() for ta in new_tasks])

                project.add_event(event_new_tasks())
                logger.info(pformat(project._events))

            logger.info("Project event added")
            _exitval_ = 0

            if submit_only:

                if new_tasks:
                    wl = create_workload_launcher(project, new_tasks, session, args, _cwd_)
                    logger.debug(pformat(wl.job_configuration))

                    if str(args.config_name).startswith("rhea"):

                        from time import sleep

                        submit_command = wl.launch_job(session, return_command=True)
                        #project.request_resource(submit_command)
                        with open("resource.request", "w") as f:
                            f.write(submit_command)

                        _exitval_ = os.path.join(session, "admd.job.state")

                    else:
                        wl.launch_job(session)
                        _exitval_ = os.path.join(session, "admd.job.state")

            else:
                logger.info("Project waiting on completion event for last workload")
                project.wait_until(project.events_done)
                logger.info("Project event done")
                logger.info("Shutting down the workers")
                project.workers.all.execute("shutdown")

    except KeyboardInterrupt:

        _exitval_ = -2

        logger.warning("AdaptiveMD KEYBOARD INTERRUPT- Quitting Workflow Execution")

    except Exception as e:

        _exitval_ = -1

        logger.error("Error during workflow: {}".format(e))
        logger.error(traceback.print_exc())

    finally:

        os.chdir(_cwd_)

        if project:
            project.resources.consume_one()
            project.close()
            logger.info("Project closed")

        if session:
            logger.info("Moving application files to session directory")
            logfiles = glob.glob("*log")
            appdir = os.path.join(session, "application")
            os.makedirs(appdir)
            for log in logfiles:
                shutil.move(log, os.path.join(appdir, log))

        logger.info("Exiting Event Script")

        print(_exitval_)
        sys.exit(_exitval_)

